{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "実行結果は別ファイル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Pix2Pix**\n",
    "<br>\n",
    "<br>「条件画像と画像のペア」を学習データとしてその対応関係を学習するもの\n",
    "<br>GeneratorとDiscriminatorがあり、\n",
    "<br>Generatorは条件画像xとノイズベクトルzから画像G(x, z)を生成。\n",
    "<br>Discriminatorは「条件画像xと実画像yのペア」と「条件画像xと生成画像G(x, z)のペア」がそれぞれ本物かどうかを識別。\n",
    "<br>この構造によって、Generatorが条件画像から本物のような画像を生成できるように画像ペアの関係を学習。\n",
    "<br>\n",
    "<br>学習にかなり時間がかかり、５エポックではあまり綺麗な画像は生成できなかった。２０くらいは必要？\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:44:29.453482Z",
     "start_time": "2020-03-10T02:44:29.444126Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pdb\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（二値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:57:04.487247Z",
     "start_time": "2020-03-10T02:57:04.477127Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = iris.data\n",
    "feature_names = iris.feature_names\n",
    "label = iris.target\n",
    "df = pd.DataFrame(data, columns=feature_names)\n",
    "df[\"target\"] = label\n",
    "df = df[df[\"target\"]!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:57:06.046834Z",
     "start_time": "2020-03-10T02:57:06.036108Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(df[\"target\"])\n",
    "X = df.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:57:06.347593Z",
     "start_time": "2020-03-10T02:57:06.340954Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:58:03.866009Z",
     "start_time": "2020-03-10T02:57:57.481566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 292\n",
      "Trainable params: 292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 0s 2ms/sample - loss: 0.6733 - acc: 0.7250\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.4744 - acc: 1.0000\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.3057 - acc: 1.0000\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.1372 - acc: 1.0000\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 401us/sample - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 318us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 452us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 310us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 310us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 326us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 410us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 669us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 441us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 926us/sample - loss: 9.8517e-04 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 9.1508e-04 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 8.5900e-04 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 7.9530e-04 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 339us/sample - loss: 7.5016e-04 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 398us/sample - loss: 7.0336e-04 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 6.5868e-04 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 6.2839e-04 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 5.7225e-04 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 479us/sample - loss: 5.4971e-04 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 5.2024e-04 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 4.9600e-04 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 277us/sample - loss: 4.7123e-04 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 4.4538e-04 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 343us/sample - loss: 4.3260e-04 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 4.0161e-04 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 3.8874e-04 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 385us/sample - loss: 3.6824e-04 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 343us/sample - loss: 3.5114e-04 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 3.4049e-04 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 3.2473e-04 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 286us/sample - loss: 3.1640e-04 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 443us/sample - loss: 3.0500e-04 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 379us/sample - loss: 2.8635e-04 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 2.7137e-04 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 2.6145e-04 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 2.5309e-04 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 411us/sample - loss: 2.4689e-04 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 2.3890e-04 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 358us/sample - loss: 2.2977e-04 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 2.1925e-04 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 2.1014e-04 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 2.0556e-04 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 1.9789e-04 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 1.9168e-04 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 1.8474e-04 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 1.8038e-04 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 1.7767e-04 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 1.6773e-04 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 1.6245e-04 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 305us/sample - loss: 1.5761e-04 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 1.5384e-04 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 1.5282e-04 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 1.4331e-04 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 1.4275e-04 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 313us/sample - loss: 1.3722e-04 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 397us/sample - loss: 1.3402e-04 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 758us/sample - loss: 1.2907e-04 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 349us/sample - loss: 1.2742e-04 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 1.2412e-04 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 1.1835e-04 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 1.1650e-04 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 1.1316e-04 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 1.1046e-04 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 1.0758e-04 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 1.0481e-04 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 1.0208e-04 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 1.0110e-04 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 9.7542e-05 - acc: 1.0000\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 302us/sample - loss: 9.5691e-05 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 517us/sample - loss: 9.3829e-05 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 9.1836e-05 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 323us/sample - loss: 8.8801e-05 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 8.8495e-05 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 326us/sample - loss: 8.5647e-05 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 8.3237e-05 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 8.1248e-05 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 8.0248e-05 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 7.8002e-05 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 7.6261e-05 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 419us/sample - loss: 7.5158e-05 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 7.3883e-05 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 7.1917e-05 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 7.0450e-05 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 6.9062e-05 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 6.7514e-05 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 6.6003e-05 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 6.5233e-05 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 6.4041e-05 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 6.3217e-05 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 6.1635e-05 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 6.0169e-05 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 5.9298e-05 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 5.8375e-05 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 5.7139e-05 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 5.5824e-05 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 5.5362e-05 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 5.4222e-05 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 5.3384e-05 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 5.2686e-05 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 5.1487e-05 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 5.0919e-05 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 5.0232e-05 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 4.8437e-05 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 409us/sample - loss: 4.7610e-05 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 4.6792e-05 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 569us/sample - loss: 4.6009e-05 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 4.5570e-05 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 4.4597e-05 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 4.3716e-05 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 4.3231e-05 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 4.2661e-05 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 4.1665e-05 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 4.1440e-05 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 4.0798e-05 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 4.0032e-05 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 658us/sample - loss: 3.9439e-05 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 294us/sample - loss: 3.9025e-05 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 288us/sample - loss: 3.8287e-05 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 3.7518e-05 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 3.6792e-05 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 3.6271e-05 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 3.5767e-05 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 3.5541e-05 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 310us/sample - loss: 3.4785e-05 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 3.4236e-05 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 3.3839e-05 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 3.3446e-05 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 3.2861e-05 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 3.2680e-05 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 3.1934e-05 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 3.1670e-05 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 288us/sample - loss: 3.1226e-05 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 3.0591e-05 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 3.0656e-05 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 2.9605e-05 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 2.9379e-05 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 2.8967e-05 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 2.8422e-05 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 2.8406e-05 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 2.7717e-05 - acc: 1.0000\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 322us/sample - loss: 2.7419e-05 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 2.6924e-05 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 2.6698e-05 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 2.6416e-05 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 2.6235e-05 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 2.5775e-05 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 302us/sample - loss: 2.5633e-05 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 2.4985e-05 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 2.4952e-05 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 2.4552e-05 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 2.4043e-05 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 2.3696e-05 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 2.3466e-05 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 2.3298e-05 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 2.2943e-05 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 2.2420e-05 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 2.2411e-05 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 2.2279e-05 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 2.1768e-05 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 2.1711e-05 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 2.1242e-05 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 2.1024e-05 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 2.0658e-05 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 2.0551e-05 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 2.0211e-05 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 2.0007e-05 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 1.9808e-05 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 1.9722e-05 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 1.9398e-05 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 1.9098e-05 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 1.8853e-05 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 1.8613e-05 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 1.8536e-05 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 1.8404e-05 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 1.8103e-05 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7941e-05 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 1.7626e-05 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 1.7431e-05 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 1.7239e-05 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 1.7144e-05 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 1.6817e-05 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 496us/sample - loss: 1.6701e-05 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 1.6434e-05 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 1.6353e-05 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 1.6154e-05 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 295us/sample - loss: 1.5936e-05 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 1.5798e-05 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 1.5599e-05 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 1.5438e-05 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5436e-05 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 1.5130e-05 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(2, activation=tf.nn.softmax)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=10,\n",
    "                    epochs=200,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:58:19.930341Z",
     "start_time": "2020-03-10T02:58:19.794486Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:58:38.528401Z",
     "start_time": "2020-03-10T02:58:38.515554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(y_pred_proba, axis=1)==np.argmax(test_y, axis=1))/len(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:07:52.312049Z",
     "start_time": "2020-03-10T02:07:52.306390Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:07:52.344873Z",
     "start_time": "2020-03-10T02:07:52.340837Z"
    }
   },
   "outputs": [],
   "source": [
    "data = iris.data\n",
    "feature_names = iris.feature_names\n",
    "label = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:08:13.295703Z",
     "start_time": "2020-03-10T02:08:13.290894Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=feature_names)\n",
    "df[\"target\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:33:11.762189Z",
     "start_time": "2020-03-10T02:33:11.746804Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(df[\"target\"])\n",
    "X = df.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:33:56.326866Z",
     "start_time": "2020-03-10T02:33:56.294711Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:44:53.936433Z",
     "start_time": "2020-03-10T02:44:47.554244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/niikurasayaka/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/200\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 0.5181 - acc: 0.7861\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.3062 - acc: 0.8750\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.2132 - acc: 0.9361\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.1570 - acc: 0.9500\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 0s 197us/sample - loss: 0.1277 - acc: 0.9556\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.1074 - acc: 0.9833\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 0.0706 - acc: 0.9778\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.0839 - acc: 0.9611\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.0869 - acc: 0.9556\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.0832 - acc: 0.9667\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 0s 214us/sample - loss: 0.0649 - acc: 0.9722\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 0s 244us/sample - loss: 0.1495 - acc: 0.9500\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 0s 290us/sample - loss: 0.0645 - acc: 0.9722\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 0s 251us/sample - loss: 0.0623 - acc: 0.9611\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 0s 279us/sample - loss: 0.1007 - acc: 0.9500\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 0s 321us/sample - loss: 0.0740 - acc: 0.9722\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.0597 - acc: 0.9778\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 0s 254us/sample - loss: 0.0795 - acc: 0.9611\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 0s 237us/sample - loss: 0.0808 - acc: 0.9667\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - 0s 249us/sample - loss: 0.0990 - acc: 0.9667\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.0524 - acc: 0.9833\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.0668 - acc: 0.9722\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 0s 171us/sample - loss: 0.0528 - acc: 0.9833\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 0s 243us/sample - loss: 0.0526 - acc: 0.9833\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 0s 277us/sample - loss: 0.0469 - acc: 0.9833\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 0s 265us/sample - loss: 0.0644 - acc: 0.9667\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 0.0516 - acc: 0.9889\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 0s 232us/sample - loss: 0.0822 - acc: 0.9667\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.0747 - acc: 0.9778\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 0s 218us/sample - loss: 0.0707 - acc: 0.9722\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.0617 - acc: 0.9722\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 0s 152us/sample - loss: 0.0540 - acc: 0.9889\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 0s 201us/sample - loss: 0.0797 - acc: 0.9722\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.0766 - acc: 0.9611\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 0.0685 - acc: 0.9778\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.0452 - acc: 0.9778\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0550 - acc: 0.9778\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.0637 - acc: 0.9833\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.0640 - acc: 0.9778\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 0s 212us/sample - loss: 0.0534 - acc: 0.9722\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 0s 220us/sample - loss: 0.0536 - acc: 0.9722\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.0784 - acc: 0.9611\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.0751 - acc: 0.9667\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.1019 - acc: 0.9611\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.1360 - acc: 0.9333\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 0s 192us/sample - loss: 0.0800 - acc: 0.9667\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 0s 161us/sample - loss: 0.0460 - acc: 0.9833\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 0s 274us/sample - loss: 0.0413 - acc: 0.9833\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 0s 180us/sample - loss: 0.0450 - acc: 0.9778\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 0s 197us/sample - loss: 0.0492 - acc: 0.9778\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 0s 267us/sample - loss: 0.0502 - acc: 0.9778\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 0s 245us/sample - loss: 0.0947 - acc: 0.9500\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 0s 247us/sample - loss: 0.0481 - acc: 0.9722\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 0s 248us/sample - loss: 0.0466 - acc: 0.9778\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.0896 - acc: 0.9667\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 0s 234us/sample - loss: 0.0959 - acc: 0.9611\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 0s 267us/sample - loss: 0.0551 - acc: 0.9778\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 0s 285us/sample - loss: 0.0402 - acc: 0.9889\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 0s 274us/sample - loss: 0.0394 - acc: 0.9833\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 0s 306us/sample - loss: 0.0408 - acc: 0.9833\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.0440 - acc: 0.9833\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 0.0324 - acc: 0.9889\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.0550 - acc: 0.9833\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.0453 - acc: 0.9833\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.0391 - acc: 0.9889\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 0s 155us/sample - loss: 0.0433 - acc: 0.9833\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.0392 - acc: 0.9833\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 0s 159us/sample - loss: 0.0517 - acc: 0.9722\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 0s 195us/sample - loss: 0.0520 - acc: 0.9722\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 0.0597 - acc: 0.9722\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 0s 156us/sample - loss: 0.0504 - acc: 0.9889\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 0s 178us/sample - loss: 0.0350 - acc: 0.9889\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.0618 - acc: 0.9667\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 0s 156us/sample - loss: 0.0488 - acc: 0.9778\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 0s 163us/sample - loss: 0.0905 - acc: 0.9722\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 0s 254us/sample - loss: 0.0898 - acc: 0.9722\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 0s 196us/sample - loss: 0.1427 - acc: 0.9500\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 0.0763 - acc: 0.9722\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 0.0526 - acc: 0.9833\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 0s 185us/sample - loss: 0.0423 - acc: 0.9778\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 0s 188us/sample - loss: 0.0534 - acc: 0.9722\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 0s 250us/sample - loss: 0.0473 - acc: 0.9833\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 0s 174us/sample - loss: 0.0543 - acc: 0.9722\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 0.0402 - acc: 0.9889\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.0383 - acc: 0.9889\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.0552 - acc: 0.9778\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 0s 190us/sample - loss: 0.0567 - acc: 0.9778\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 0s 161us/sample - loss: 0.0655 - acc: 0.9722\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.0425 - acc: 0.9778\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 0s 172us/sample - loss: 0.0434 - acc: 0.9889\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.0898 - acc: 0.9556\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 0s 173us/sample - loss: 0.0774 - acc: 0.9611\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 0s 196us/sample - loss: 0.0484 - acc: 0.9833\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.0462 - acc: 0.9833\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0400 - acc: 0.9778\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.0381 - acc: 0.9833\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.0537 - acc: 0.9778\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.0498 - acc: 0.9833\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.0330 - acc: 0.9889\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 0s 222us/sample - loss: 0.0336 - acc: 0.9889\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 0s 260us/sample - loss: 0.0452 - acc: 0.9722\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 0s 171us/sample - loss: 0.0422 - acc: 0.9833\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0435 - acc: 0.9833\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.0327 - acc: 0.9889\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 0s 152us/sample - loss: 0.0376 - acc: 0.9833\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.0364 - acc: 0.9833\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.0713 - acc: 0.9722\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0831 - acc: 0.933 - 0s 252us/sample - loss: 0.0324 - acc: 0.9889\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 0s 540us/sample - loss: 0.0594 - acc: 0.9778\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 0s 446us/sample - loss: 0.0359 - acc: 0.9889\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.0356 - acc: 0.9833\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 0s 213us/sample - loss: 0.0302 - acc: 0.9889\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 0s 291us/sample - loss: 0.0471 - acc: 0.9778\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 0s 251us/sample - loss: 0.0313 - acc: 0.9889\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 0s 218us/sample - loss: 0.0424 - acc: 0.9889\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.0919 - acc: 0.9722\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.0502 - acc: 0.9722\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.0362 - acc: 0.9944\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 0s 174us/sample - loss: 0.0354 - acc: 0.9889\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 0s 188us/sample - loss: 0.0374 - acc: 0.9833\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 0s 172us/sample - loss: 0.0457 - acc: 0.9778\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 0s 205us/sample - loss: 0.0307 - acc: 0.9944\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.0291 - acc: 0.9944\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 0s 289us/sample - loss: 0.0339 - acc: 0.9889\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 0s 271us/sample - loss: 0.0461 - acc: 0.9833\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.0322 - acc: 0.9889\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0355 - acc: 0.9833\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.0411 - acc: 0.9833\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 0s 195us/sample - loss: 0.0369 - acc: 0.9833\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.0364 - acc: 0.9889\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 0s 204us/sample - loss: 0.0352 - acc: 0.9833\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.0308 - acc: 0.9889\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.0362 - acc: 0.9889\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 0s 226us/sample - loss: 0.0394 - acc: 0.9833\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.0569 - acc: 0.9722\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 0s 181us/sample - loss: 0.0954 - acc: 0.9611\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 0s 171us/sample - loss: 0.1286 - acc: 0.9444\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 0s 163us/sample - loss: 0.0688 - acc: 0.9778\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 0s 179us/sample - loss: 0.0383 - acc: 0.9889\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 0s 209us/sample - loss: 0.0364 - acc: 0.9889\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 0s 196us/sample - loss: 0.0350 - acc: 0.9889\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 0s 172us/sample - loss: 0.0350 - acc: 0.9889\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.0328 - acc: 0.9889\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 0s 178us/sample - loss: 0.0382 - acc: 0.9889\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 0s 187us/sample - loss: 0.0389 - acc: 0.9833\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 0s 152us/sample - loss: 0.0516 - acc: 0.9778\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.0377 - acc: 0.9889\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 180us/sample - loss: 0.0440 - acc: 0.9778\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 0s 144us/sample - loss: 0.0431 - acc: 0.9750\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.0779 - acc: 0.9667\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 0s 198us/sample - loss: 0.0570 - acc: 0.9722\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.0640 - acc: 0.9667\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.0390 - acc: 0.9889\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.0350 - acc: 0.9889\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 0s 181us/sample - loss: 0.0403 - acc: 0.9889\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 0s 236us/sample - loss: 0.0349 - acc: 0.9889\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.0377 - acc: 0.9778\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.0314 - acc: 0.9944\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 0s 187us/sample - loss: 0.0351 - acc: 0.9889\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0531 - acc: 0.9722\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 0s 184us/sample - loss: 0.0306 - acc: 0.9944\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.0609 - acc: 0.9750\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 0s 209us/sample - loss: 0.0408 - acc: 0.9889\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.0392 - acc: 0.9778\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 0s 231us/sample - loss: 0.0530 - acc: 0.9778\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 0s 188us/sample - loss: 0.0422 - acc: 0.9778\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.0435 - acc: 0.9778\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 0s 252us/sample - loss: 0.0582 - acc: 0.9722\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.0364 - acc: 0.9833\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.0358 - acc: 0.9889\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 0s 207us/sample - loss: 0.0332 - acc: 0.9889\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 0s 228us/sample - loss: 0.0394 - acc: 0.9889\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 0s 222us/sample - loss: 0.0413 - acc: 0.9889\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.0542 - acc: 0.9722\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 0s 192us/sample - loss: 0.0616 - acc: 0.9722\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.0420 - acc: 0.9722\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 0s 254us/sample - loss: 0.0386 - acc: 0.9778\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 0s 269us/sample - loss: 0.0380 - acc: 0.9778\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 0s 259us/sample - loss: 0.0557 - acc: 0.9611\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 0s 252us/sample - loss: 0.0586 - acc: 0.9722\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 0s 201us/sample - loss: 0.0330 - acc: 0.9889\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.0349 - acc: 0.9833\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.0374 - acc: 0.9889\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 0s 174us/sample - loss: 0.0348 - acc: 0.9778\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 0.0308 - acc: 0.9944\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 0.0320 - acc: 0.9833\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.0451 - acc: 0.9778\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 0s 179us/sample - loss: 0.0504 - acc: 0.9722\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 0s 218us/sample - loss: 0.0532 - acc: 0.9833\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 0s 178us/sample - loss: 0.0765 - acc: 0.9667\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 0s 209us/sample - loss: 0.0488 - acc: 0.9778\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 0s 205us/sample - loss: 0.0378 - acc: 0.9889\n",
      "Epoch 193/200\n",
      "120/120 [==============================] - 0s 266us/sample - loss: 0.0407 - acc: 0.9833\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 0s 247us/sample - loss: 0.0384 - acc: 0.9889\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.0330 - acc: 0.9889\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 0s 195us/sample - loss: 0.0337 - acc: 0.9833\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 0s 180us/sample - loss: 0.0417 - acc: 0.9833\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 0s 173us/sample - loss: 0.0421 - acc: 0.9889\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.0268 - acc: 0.9833\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 0s 275us/sample - loss: 0.0322 - acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=10,\n",
    "                    epochs=200,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:49:03.154531Z",
     "start_time": "2020-03-10T02:49:03.141241Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:49:16.575063Z",
     "start_time": "2020-03-10T02:49:16.557637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(y_pred_proba, axis=1)==np.argmax(test_y, axis=1))/len(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:47:44.350854Z",
     "start_time": "2020-03-10T03:47:44.348051Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:00:36.842607Z",
     "start_time": "2020-03-10T03:00:36.809819Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/niikurasayaka/diveintocode-ml/Week3/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:46:00.322173Z",
     "start_time": "2020-03-10T03:46:00.301981Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df[\"SalePrice\"]\n",
    "y = np.log(np.array(y))\n",
    "y = np.reshape(y, [-1, 1])\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
    "       '1stFlrSF', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:47:46.903633Z",
     "start_time": "2020-03-10T03:47:46.889309Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "sc = StandardScaler()\n",
    "sc.fit(train_x)\n",
    "train_x = sc.transform(train_x)\n",
    "test_x = sc.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:57:25.859873Z",
     "start_time": "2020-03-10T03:57:22.282009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/80\n",
      "1168/1168 [==============================] - 0s 265us/sample - loss: 100.0866 - acc: 0.0000e+00 - val_loss: 44.2052 - val_acc: 0.0000e+00\n",
      "Epoch 2/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 29.9739 - acc: 0.0000e+00 - val_loss: 19.4636 - val_acc: 0.0000e+00\n",
      "Epoch 3/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 14.0944 - acc: 0.0000e+00 - val_loss: 9.0331 - val_acc: 0.0000e+00\n",
      "Epoch 4/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 7.4476 - acc: 0.0000e+00 - val_loss: 5.0797 - val_acc: 0.0000e+00\n",
      "Epoch 5/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 4.7259 - acc: 0.0000e+00 - val_loss: 3.0076 - val_acc: 0.0000e+00\n",
      "Epoch 6/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 3.3422 - acc: 0.0000e+00 - val_loss: 2.3942 - val_acc: 0.0000e+00\n",
      "Epoch 7/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 2.6020 - acc: 0.0000e+00 - val_loss: 2.0292 - val_acc: 0.0000e+00\n",
      "Epoch 8/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 2.1891 - acc: 0.0000e+00 - val_loss: 1.7818 - val_acc: 0.0000e+00\n",
      "Epoch 9/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 1.8030 - acc: 0.0000e+00 - val_loss: 1.5138 - val_acc: 0.0000e+00\n",
      "Epoch 10/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1.5481 - acc: 0.0000e+00 - val_loss: 1.3028 - val_acc: 0.0000e+00\n",
      "Epoch 11/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1.3055 - acc: 0.0000e+00 - val_loss: 1.1765 - val_acc: 0.0000e+00\n",
      "Epoch 12/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1.0947 - acc: 0.0000e+00 - val_loss: 0.9699 - val_acc: 0.0000e+00\n",
      "Epoch 13/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 0.9252 - acc: 0.0000e+00 - val_loss: 0.8497 - val_acc: 0.0000e+00\n",
      "Epoch 14/80\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 0.7890 - acc: 0.0000e+00 - val_loss: 0.6916 - val_acc: 0.0000e+00\n",
      "Epoch 15/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 0.6452 - acc: 0.0000e+00 - val_loss: 0.5919 - val_acc: 0.0000e+00\n",
      "Epoch 16/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.5456 - acc: 0.0000e+00 - val_loss: 0.4620 - val_acc: 0.0000e+00\n",
      "Epoch 17/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.4622 - acc: 0.0000e+00 - val_loss: 0.4055 - val_acc: 0.0000e+00\n",
      "Epoch 18/80\n",
      "1168/1168 [==============================] - 0s 47us/sample - loss: 0.3560 - acc: 0.0000e+00 - val_loss: 0.3498 - val_acc: 0.0000e+00\n",
      "Epoch 19/80\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 0.3041 - acc: 0.0000e+00 - val_loss: 0.2695 - val_acc: 0.0000e+00\n",
      "Epoch 20/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 0.2654 - acc: 0.0000e+00 - val_loss: 0.2362 - val_acc: 0.0000e+00\n",
      "Epoch 21/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.2258 - acc: 0.0000e+00 - val_loss: 0.2287 - val_acc: 0.0000e+00\n",
      "Epoch 22/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.1997 - acc: 0.0000e+00 - val_loss: 0.1971 - val_acc: 0.0000e+00\n",
      "Epoch 23/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1612 - val_acc: 0.0000e+00\n",
      "Epoch 24/80\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 0.1475 - acc: 0.0000e+00 - val_loss: 0.1431 - val_acc: 0.0000e+00\n",
      "Epoch 25/80\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 0.1347 - acc: 0.0000e+00 - val_loss: 0.1350 - val_acc: 0.0000e+00\n",
      "Epoch 26/80\n",
      "1168/1168 [==============================] - 0s 32us/sample - loss: 0.1184 - acc: 0.0000e+00 - val_loss: 0.1183 - val_acc: 0.0000e+00\n",
      "Epoch 27/80\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 0.1101 - acc: 0.0000e+00 - val_loss: 0.1106 - val_acc: 0.0000e+00\n",
      "Epoch 28/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.1043 - acc: 0.0000e+00 - val_loss: 0.1095 - val_acc: 0.0000e+00\n",
      "Epoch 29/80\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 0.0946 - acc: 0.0000e+00 - val_loss: 0.0964 - val_acc: 0.0000e+00\n",
      "Epoch 30/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.0866 - acc: 0.0000e+00 - val_loss: 0.0950 - val_acc: 0.0000e+00\n",
      "Epoch 31/80\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 0.0833 - acc: 0.0000e+00 - val_loss: 0.0926 - val_acc: 0.0000e+00\n",
      "Epoch 32/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.0788 - acc: 0.0000e+00 - val_loss: 0.0881 - val_acc: 0.0000e+00\n",
      "Epoch 33/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0730 - acc: 0.0000e+00 - val_loss: 0.0785 - val_acc: 0.0000e+00\n",
      "Epoch 34/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0697 - acc: 0.0000e+00 - val_loss: 0.0739 - val_acc: 0.0000e+00\n",
      "Epoch 35/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 0.0658 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
      "Epoch 36/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0627 - acc: 0.0000e+00 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
      "Epoch 37/80\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 0.0599 - acc: 0.0000e+00 - val_loss: 0.0669 - val_acc: 0.0000e+00\n",
      "Epoch 38/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0583 - acc: 0.0000e+00 - val_loss: 0.0651 - val_acc: 0.0000e+00\n",
      "Epoch 39/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.0574 - acc: 0.0000e+00 - val_loss: 0.0629 - val_acc: 0.0000e+00\n",
      "Epoch 40/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.0555 - acc: 0.0000e+00 - val_loss: 0.0614 - val_acc: 0.0000e+00\n",
      "Epoch 41/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0523 - acc: 0.0000e+00 - val_loss: 0.0573 - val_acc: 0.0000e+00\n",
      "Epoch 42/80\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 0.0512 - acc: 0.0000e+00 - val_loss: 0.0551 - val_acc: 0.0000e+00\n",
      "Epoch 43/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0496 - acc: 0.0000e+00 - val_loss: 0.0524 - val_acc: 0.0000e+00\n",
      "Epoch 44/80\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 0.0484 - acc: 0.0000e+00 - val_loss: 0.0537 - val_acc: 0.0000e+00\n",
      "Epoch 45/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0479 - acc: 0.0000e+00 - val_loss: 0.0518 - val_acc: 0.0000e+00\n",
      "Epoch 46/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 0.0457 - acc: 0.0000e+00 - val_loss: 0.0502 - val_acc: 0.0000e+00\n",
      "Epoch 47/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 0.0455 - acc: 0.0000e+00 - val_loss: 0.0472 - val_acc: 0.0000e+00\n",
      "Epoch 48/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0441 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0000e+00\n",
      "Epoch 49/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0436 - acc: 0.0000e+00 - val_loss: 0.0437 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0409 - acc: 0.0000e+00 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 51/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0391 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
      "Epoch 52/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0388 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 53/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0376 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0000e+00\n",
      "Epoch 54/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0377 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 55/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0393 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0000e+00\n",
      "Epoch 56/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0370 - acc: 0.0000e+00 - val_loss: 0.0419 - val_acc: 0.0000e+00\n",
      "Epoch 57/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0379 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 58/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
      "Epoch 59/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0338 - acc: 0.0000e+00 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
      "Epoch 60/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0351 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0000e+00\n",
      "Epoch 61/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0333 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 62/80\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 0.0312 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
      "Epoch 63/80\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 0.0337 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 64/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0317 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 65/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0308 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
      "Epoch 66/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0306 - acc: 0.0000e+00 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 67/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0308 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0000e+00\n",
      "Epoch 68/80\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 0.0324 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 69/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0361 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 70/80\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 0.0308 - acc: 0.0000e+00 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
      "Epoch 71/80\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 0.0300 - acc: 0.0000e+00 - val_loss: 0.0309 - val_acc: 0.0000e+00\n",
      "Epoch 72/80\n",
      "1168/1168 [==============================] - 0s 30us/sample - loss: 0.0303 - acc: 0.0000e+00 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 73/80\n",
      "1168/1168 [==============================] - 0s 29us/sample - loss: 0.0299 - acc: 0.0000e+00 - val_loss: 0.0336 - val_acc: 0.0000e+00\n",
      "Epoch 74/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0281 - acc: 0.0000e+00 - val_loss: 0.0309 - val_acc: 0.0000e+00\n",
      "Epoch 75/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 76/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0288 - acc: 0.0000e+00 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
      "Epoch 77/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0289 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 78/80\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 0.0304 - acc: 0.0000e+00 - val_loss: 0.0319 - val_acc: 0.0000e+00\n",
      "Epoch 79/80\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 0.0282 - acc: 0.0000e+00 - val_loss: 0.0329 - val_acc: 0.0000e+00\n",
      "Epoch 80/80\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 0.0298 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(9,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=100,\n",
    "                    epochs=80,\n",
    "                    validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T03:59:09.753629Z",
     "start_time": "2020-03-10T03:59:09.749212Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:00:20.235599Z",
     "start_time": "2020-03-10T04:00:18.856943Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:01:08.292874Z",
     "start_time": "2020-03-10T04:01:08.139794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14091a890>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss)), loss)\n",
    "plt.plot(range(len(val_loss)), val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:01:44.045846Z",
     "start_time": "2020-03-10T04:01:43.651140Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:01:44.753364Z",
     "start_time": "2020-03-10T04:01:44.358620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:01:45.118347Z",
     "start_time": "2020-03-10T04:01:45.084304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:01:55.112854Z",
     "start_time": "2020-03-10T04:01:54.844432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(12000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:42:10.285773Z",
     "start_time": "2020-03-10T04:42:10.264462Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "X_val = np.reshape(X_val, (-1, 28, 28, 1))\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:26:54.442090Z",
     "start_time": "2020-03-10T04:26:54.435395Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Activation, Conv2D, Flatten, Dense,Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam\n",
    "from keras import Model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:46:21.685364Z",
     "start_time": "2020-03-10T04:43:34.093350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                200768    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 220,234\n",
      "Trainable params: 220,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 35s 722us/step - loss: 0.7704 - accuracy: 0.7857 - val_loss: 0.2553 - val_accuracy: 0.9251\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 33s 681us/step - loss: 0.2090 - accuracy: 0.9364 - val_loss: 0.1970 - val_accuracy: 0.9399\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 0.1447 - accuracy: 0.9568 - val_loss: 0.1332 - val_accuracy: 0.9593\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 33s 687us/step - loss: 0.1131 - accuracy: 0.9659 - val_loss: 0.1126 - val_accuracy: 0.9654\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 0.0937 - accuracy: 0.9719 - val_loss: 0.0929 - val_accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(filters = 32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(input_data)\n",
    "x = Activation(activation = \"relu\")(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation(activation = \"relu\")(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation(activation = \"relu\")(x)\n",
    "x = Dense(10)(x)\n",
    "output = Activation(activation = \"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=SGD(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=5,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:48:20.059118Z",
     "start_time": "2020-03-10T04:48:19.882187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14255c690>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.plot(range(len(val_loss)), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:50:39.227488Z",
     "start_time": "2020-03-10T04:50:37.291162Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:51:47.495582Z",
     "start_time": "2020-03-10T04:51:47.488989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(pred, axis=1)==np.argmax(y_test_one_hot, axis=1))/len(pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T04:51:35.037196Z",
     "start_time": "2020-03-10T04:51:35.031673Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
